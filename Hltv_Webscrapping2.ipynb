{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables to store and keep data.\n",
    "#match_links = []\n",
    "df_list=[]\n",
    "matches_scrapped=[]\n",
    "df_picks_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get Match links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all the match links from hltv results page!\n",
    "# Obs: as the project have focus on A teams, we are using Two star matchs filter.\n",
    "\n",
    "for x in range(0,1000,100):\n",
    "    url = f'https://www.hltv.org/results?offset={x}&stars=2'\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    for i in range(len(soup.find_all('div', {'class':'result-con'}))):\n",
    "        if (soup.find_all('div', {'class':'result-con'})[i]('a')[0]['href']) in match_links:\n",
    "            pass\n",
    "        else:match_links.append(soup.find_all('div', {'class':'result-con'})[i]('a')[0]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 1000 matches we gonna scrap\n",
    "len(match_links)\n",
    "len(matches_scrapped)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 901/901 [1:02:14<00:00,  4.15s/it]\n"
     ]
    }
   ],
   "source": [
    "matches_dict = { \n",
    "    'match_id' : [],\n",
    "    'date' : [],\n",
    "    'team_1' : [],\n",
    "    'score_1' : [],\n",
    "    'score_2' : [],\n",
    "    'team_2' :[],\n",
    "    'bestof' :[],\n",
    "    'environment' : [],\n",
    "    'tournment' :[],\n",
    "    'map_1' : [],\n",
    "    'map_2' : [],\n",
    "    'map_3' : []\n",
    "    }\n",
    "df_list = []\n",
    "matches_scrapped = []\n",
    "for item in tqdm(match_links[0:901]):\n",
    "        url = f'https://www.hltv.org{item}'\n",
    "        response = requests.get(url)\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html)\n",
    "        if url in matches_scrapped:\n",
    "            pass\n",
    "        elif re.findall('\\west of [0-9]',soup.find_all('div',{'class':'padding preformatted-text'})[0].text)[0] == 'Best of 5':\n",
    "            pass\n",
    "        else:\n",
    "            try: \n",
    "                #match_id\n",
    "                #match_id = (re.findall('\\d+',url)[0])\n",
    "                #date\n",
    "                #date = (soup.find_all('div',{'class':'timeAndEvent'})[0]('div',{'class':'date'})[0].text)\n",
    "                #team1 name \n",
    "                #team_1 = (soup.find_all('div',{'class':'team1-gradient'})[0]('a')[0]('img')[0]['alt'])\n",
    "                #team1 score\n",
    "                #team1_score = (soup.find_all('div',{'class':'team1-gradient'})[0]('div')[1].text)\n",
    "                #team1_id\n",
    "                #print(re.findall('\\d+',soup.find_all('div',{'class':'team1-gradient'})[0]('a')[0]['href']))\n",
    "                #team2 score\n",
    "                #team_2_score = (soup.find_all('div',{'class':'team2-gradient'})[0]('div')[1].text)\n",
    "                #team2 name\n",
    "                #team_2 = (soup.find_all('div',{'class':'team2-gradient'})[0]('a')[0]('img')[0]['alt'])\n",
    "                #bestof\n",
    "                #bof = (re.findall('\\west of [0-9]',soup.find_all('div',{'class':'padding preformatted-text'})[0].text)[0])\n",
    "                #kind of match\n",
    "                #environment = (re.findall('(\\w+)\\)',soup.find_all('div',{'class':'padding preformatted-text'})[0].text)[0])\n",
    "                #tournment\n",
    "                #tournment = (soup.find_all('div',{'class':'timeAndEvent'})[0]('div',{'class':'event text-ellipsis'})[0]('a')[0]['title'])\n",
    "                #map1\n",
    "                #map_1 = (soup.find_all('div', {'class':'flexbox-column'})[0]('div',{'class':'mapname'})[0].text)\n",
    "                #try:\n",
    "                    #map2\n",
    "                #    map_2 = (soup.find_all('div', {'class':'flexbox-column'})[0]('div',{'class':'mapname'})[1].text)\n",
    "                #except:\n",
    "                #     map_2 = ('')\n",
    "                #try:\n",
    "                    #map3\n",
    "                #    map_3 = (soup.find_all('div', {'class':'flexbox-column'})[0]('div',{'class':'mapname'})[2].text)\n",
    "                #except:\n",
    "                #    map_3 = ('')\n",
    "                matches_scrapped.append(url)\n",
    "                time.sleep(3)\n",
    "\n",
    "                #matches_dict['match_id'].append(match_id)\n",
    "                #matches_dict['date'].append(date)\n",
    "                #matches_dict['team_1'].append(team_1)                \n",
    "                #matches_dict['score_1'].append(team1_score)\n",
    "                #matches_dict['score_2'].append(team_2_score)\n",
    "                #matches_dict['team_2'].append(team_2)\n",
    "                #matches_dict['bestof'].append(bof)\n",
    "                #matches_dict['environment'].append(environment)\n",
    "                #matches_dict['tournment'].append(tournment)\n",
    "                #matches_dict['map_1'].append(map_1)\n",
    "                #matches_dict['map_2'].append(map_2)\n",
    "                #matches_dict['map_3'].append(map_3)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "#df_list.append(pd.DataFrame(matches_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat(df_list)\n",
    "\n",
    "#df.to_csv('300.600match_links.csv')\n",
    "\n",
    "#df_list = []\n",
    "\n",
    "#match_links[601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#match_links[:3]\n",
    "\n",
    "#len(matches_scrapped)\n",
    "#url\n",
    "#\n",
    "#if url in matches_scrapped:\n",
    "#    print('ok')\n",
    "#for key in matches_dict:\n",
    "#    print(key,len(matches_dict[key]))\n",
    "#for item in tqdm(match_links[:100]):\n",
    "#    print(item)\n",
    "\n",
    "#df= pd.DataFrame(matches_dict)\n",
    "#df_list[0]\n",
    "#match_links[492]\n",
    "#df_list[0]\n",
    "#df_picks_list = []\n",
    "#df=pd.concat(df_list)\n",
    "#df\n",
    "len(matches_scrapped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get Teams veto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [05:29<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "picks_bans = {\n",
    "    'match_id': [],\n",
    "    'team_ID_1' : [],\n",
    "    'team_1': [],\n",
    "    'team_ID_2': [],\n",
    "    'team_2':[],\n",
    "    'choice_1': [],\n",
    "    'choice_2': [],\n",
    "    'choice_3' : [],\n",
    "    'choice_4' : [],\n",
    "    'choice_5' : [],\n",
    "    'choice_6' : [],\n",
    "    'choice_7' : []\n",
    "}\n",
    "\n",
    "for item in tqdm(matches_scrapped):\n",
    "    url = f'{item}'\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    if re.findall('\\d+',url)[0] in picks_bans.values():\n",
    "        pass\n",
    "    else:\n",
    "        #match_id\n",
    "        picks_bans['match_id'].append(re.findall('\\d+',url)[0])\n",
    "        #team_1ID\n",
    "        picks_bans['team_ID_1'].append(re.findall('\\d+',soup.find_all('div',{'class':'team1-gradient'})[0]('a')[0]['href'])[0])\n",
    "        #team_1name\n",
    "        picks_bans['team_1'].append(soup.find_all('div',{'class':'team1-gradient'})[0]('a')[0]('img')[0]['alt'])\n",
    "        #team_2ID\n",
    "        picks_bans['team_ID_2'].append(re.findall('\\d+',soup.find_all('div',{'class':'team2-gradient'})[0]('a')[0]['href'])[0])\n",
    "        #team_2name\n",
    "        picks_bans['team_2'].append(soup.find_all('div',{'class':'team2-gradient'})[0]('a')[0]('img')[0]['alt'])\n",
    "        try:    \n",
    "            #choice1\n",
    "            picks_bans['choice_1'].append(soup.find_all('div',{'class':'standard-box veto-box'})[1].text.split('\\n')[2])\n",
    "            #choice2\n",
    "            picks_bans['choice_2'].append(soup.find_all('div',{'class':'standard-box veto-box'})[1].text.split('\\n')[3])\n",
    "            #choice3\n",
    "            picks_bans['choice_3'].append(soup.find_all('div',{'class':'standard-box veto-box'})[1].text.split('\\n')[4])\n",
    "            #choice4\n",
    "            picks_bans['choice_4'].append(soup.find_all('div',{'class':'standard-box veto-box'})[1].text.split('\\n')[5])\n",
    "            #choice5\n",
    "            picks_bans['choice_5'].append(soup.find_all('div',{'class':'standard-box veto-box'})[1].text.split('\\n')[6])\n",
    "            #choice6\n",
    "            picks_bans['choice_6'].append(soup.find_all('div',{'class':'standard-box veto-box'})[1].text.split('\\n')[7])\n",
    "            #choice7\n",
    "            picks_bans['choice_7'].append(soup.find_all('div',{'class':'standard-box veto-box'})[1].text.split('\\n')[8])\n",
    "        except:\n",
    "            picks_bans['choice_1'].append('')\n",
    "            picks_bans['choice_2'].append('')\n",
    "            picks_bans['choice_3'].append('')\n",
    "            picks_bans['choice_4'].append('')\n",
    "            picks_bans['choice_5'].append('')\n",
    "            picks_bans['choice_6'].append('')\n",
    "            picks_bans['choice_7'].append('')\n",
    "        \n",
    "df_picks_list.append(pd.DataFrame(picks_bans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_picks_list)\n",
    "\n",
    "#df_picks_list[0]\n",
    "\n",
    "df.to_csv('600.900picks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get map details for each match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 879/879 [17:07<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "map_links = {\n",
    "    'match_id': [],\n",
    "    'map1_link':[],\n",
    "    'map2_link': [],\n",
    "    'map3_link':[]\n",
    "}\n",
    "\n",
    "for item in tqdm(matches_scrapped):\n",
    "    url = f'{item}'\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    if re.findall('\\d+',url)[0] in map_links['match_id']:\n",
    "        pass\n",
    "    else:\n",
    "        #match_id\n",
    "        map_links['match_id'].append(re.findall('\\d+',url)[0])\n",
    "        try:\n",
    "            #map1_link\n",
    "            map_links['map1_link'].append(soup.find_all('div',{'class':'results-center-stats'})[0]('a')[0]['href'])\n",
    "        except:\n",
    "            map_links['map1_link'].append('')\n",
    "        try:\n",
    "            #map2_link\n",
    "            map_links['map2_link'].append(soup.find_all('div',{'class':'results-center-stats'})[1]('a')[0]['href'])\n",
    "        except:\n",
    "            map_links['map2_link'].append('')\n",
    "        try:\n",
    "        #map3_link\n",
    "            map_links['map3_link'].append(soup.find_all('div',{'class':'results-center-stats'})[2]('a')[0]['href'])\n",
    "        except:\n",
    "            map_links['map3_link'].append('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def economy(x):\n",
    "    index = x.find('mapstatsid')\n",
    "    final_string = x[:index] + 'economy/' + x[index:]\n",
    "    return final_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/stats/matches/mapstatsid/144660/g2-vs-natus-vincere'"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_map_links = pd.DataFrame(map_links)\n",
    "#list_one = list(df_map_links['map1_link'])\n",
    "#len(list_one)\n",
    "map_details_links = [ df_map_links[item] .apply(lambda x: x)for item in df_map_links]\n",
    "\n",
    "#df_map_links.to_csv('map_detail_links.csv')\n",
    "\n",
    "#df_map_links['economy3'] = df_map_links['map3_link'].apply(lambda x : economy(x) )\n",
    "\n",
    "\n",
    "df_map_links['economy1'].apply(lambda x : '' if (x == 'economy/') else x )\n",
    "\n",
    "#my_string = '/stats/matches/mapstatsid/144643/g2-vs-natus-vincere'\n",
    "#index = my_string.find('mapstatsid')\n",
    "#final_string = my_string[:index] + 'economy/' + my_string[index:]\n",
    "#print(final_string)\n",
    "\n",
    "#df_map_links.drop(index=54,inplace=True)\n",
    "#df_map_links.drop(index=243,inplace=True)\n",
    "#df_map_links[df_map_links['economy1'] == 'economy/']\n",
    "#df_map_links['economy1'].iloc[54]\n",
    "\n",
    "map_details_links[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating empty variables to append informations.\n",
    "map_details = {\n",
    "    'date' : [],\n",
    "    'match_id' : [],\n",
    "    'team_1' : [],\n",
    "    'team_2' : [],\n",
    "    'best_of' : [],\n",
    "    'map': [],\n",
    "    'score_t1' : [],\n",
    "    'ct_side_t1': [],\n",
    "    't_side_t1' : [],\n",
    "    'score_t2' : [],\n",
    "    'ct_side_t2' : [],\n",
    "    't_side_t2' : [],\n",
    "    'team_start_ct' : [],\n",
    "    'team_1_rating':[],\n",
    "    'team_2_rating':[],\n",
    "    'team_1_first_kill' : [],\n",
    "    'team_2_first_kill' : [],\n",
    "    'team_1_clutches' : [],\n",
    "    'team_2_clutches':[],\n",
    "    'eco_t1' : [],\n",
    "    'eco_t2' : [],\n",
    "    'semi_eco_t1' : [],\n",
    "    'semi_eco_t2' : [],\n",
    "    'semi_buy_t1' : [],\n",
    "    'semi_buy_t2' : [],\n",
    "    'full_buy_t1' : [],\n",
    "    'full_buy_t2' : [],    \n",
    "}\n",
    "\n",
    "for item in tqdm(map_details_links[3]):\n",
    "    url =f'https://www.hltv.org{item}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: \n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html)\n",
    "        try:\n",
    "            #date\n",
    "            date = (soup.find_all('div', {'class':'match-info-box'})[0]('span')[0].text)\n",
    "            #match_id\n",
    "            match_id = (re.findall('\\d+',soup.find_all('a', {'class':'match-page-link button'})[0]['href'])[0])\n",
    "            #team_1\n",
    "            team_1 = (soup.find_all('div', {'class':'team-left'})[0]('img')[0]['title'])\n",
    "            #team_2\n",
    "            team_2 = (soup.find_all('div', {'class':'team-right'})[0]('img')[0]['title'])\n",
    "            #map\n",
    "            map_ = (soup.find('div', {'class':'small-text'}).next_sibling.strip())\n",
    "            #score_t1\n",
    "            score_t1 = (soup.find_all('div',{'class':'match-info-row'})[0]('span')[0].text)\n",
    "            #score_t2\n",
    "            score_t2 = (soup.find_all('div',{'class':'match-info-row'})[0]('span')[1].text)\n",
    "            for i in range(2,6,2):\n",
    "                if soup.find_all('div',{'class':'match-info-row'})[0]('span')[i]['class'][0] ==  'ct-color':\n",
    "                    ct_side_t1 = (soup.find_all('div',{'class':'match-info-row'})[0]('span')[i].text)\n",
    "                else:\n",
    "                    t_side_t1 = (soup.find_all('div',{'class':'match-info-row'})[0]('span')[i].text)\n",
    "\n",
    "            for i in range(3,6,2):\n",
    "                if soup.find_all('div',{'class':'match-info-row'})[0]('span')[i]['class'][0] ==  'ct-color':\n",
    "                    ct_side_t2 = (soup.find_all('div',{'class':'match-info-row'})[0]('span')[i].text)\n",
    "                else:\n",
    "                    t_side_t2 = (soup.find_all('div',{'class':'match-info-row'})[0]('span')[i].text)\n",
    "            #team_start_ct\n",
    "            if soup.find_all('div',{'class':'match-info-row'})[0]('span')[2]['class'][0] == 'ct-color':\n",
    "                team_starts_ct = (1)\n",
    "            else:\n",
    "                team_starts_ct = (2)\n",
    "            #best of\n",
    "            try:\n",
    "                best_of = (soup.find_all('div', {'class':'stats-match-map-result-mapname dynamic-map-name-short'})[0].text)\n",
    "            except:\n",
    "                best_of = ('bo1')\n",
    "            #Team1_rating\n",
    "            team1_rating = soup.find_all('div',{'class':'match-info-row'})[1]('div', {'class':'right'})[0].text.split()[0]\n",
    "            #Team2_rating \n",
    "            team2_rating = soup.find_all('div',{'class':'match-info-row'})[1]('div', {'class':'right'})[0].text.split()[2]\n",
    "            #Team 1 first kill\n",
    "            team1_first_kill = soup.find_all('div',{'class':'match-info-row'})[2]('div', {'class':'right'})[0].text.split()[0]\n",
    "            #Team 2 first kill\n",
    "            team2_first_kill = soup.find_all('div',{'class':'match-info-row'})[2]('div', {'class':'right'})[0].text.split()[2]\n",
    "            #t1_clutches\n",
    "            team1_clutches = soup.find_all('div',{'class':'match-info-row'})[3]('div', {'class':'right'})[0].text.split()[0]\n",
    "            #t2 Clutches\n",
    "            team2_clutches = soup.find_all('div',{'class':'match-info-row'})[3]('div', {'class':'right'})[0].text.split()[2]\n",
    "        \n",
    "            tempurl2 = soup.find_all('div',{'class':'tabs'})[0]('a',{'class':'stats-top-menu-item stats-top-menu-item-link'})[1]['href']\n",
    "            time.sleep(3)\n",
    "\n",
    "            url2 =f'https://www.hltv.org{tempurl2}'\n",
    "            response = requests.get(url2)\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html)\n",
    "            \n",
    "            eco_t1 = soup.find_all('div',{'class':'col standard-box stats-rows'})[0]('span',{'title':'Played'})[0].text.split()[0]\n",
    "            eco_t2 = soup.find_all('div',{'class':'col standard-box stats-rows'})[1]('span',{'title':'Played'})[0].text.split()[0]\n",
    "            semi_eco_t1 = soup.find_all('div',{'class':'col standard-box stats-rows'})[0]('span',{'title':'Played'})[1].text.split()[0]\n",
    "            semi_eco_t2 = soup.find_all('div',{'class':'col standard-box stats-rows'})[1]('span',{'title':'Played'})[1].text.split()[0]\n",
    "            semi_buy_t1 = soup.find_all('div',{'class':'col standard-box stats-rows'})[0]('span',{'title':'Played'})[2].text.split()[0]\n",
    "            semi_buy_t2 = soup.find_all('div',{'class':'col standard-box stats-rows'})[1]('span',{'title':'Played'})[2].text.split()[0]\n",
    "            full_buy_t1 = soup.find_all('div',{'class':'col standard-box stats-rows'})[0]('span',{'title':'Played'})[3].text.split()[0]\n",
    "            full_buy_t2 = soup.find_all('div',{'class':'col standard-box stats-rows'})[1]('span',{'title':'Played'})[3].text.split()[0]\n",
    "            map_details['date'].append(date)            \n",
    "            map_details['match_id'].append(match_id)\n",
    "            map_details['team_1'].append(team_1)\n",
    "            map_details['score_t1'].append(score_t1)\n",
    "            map_details['ct_side_t1'].append(ct_side_t1)\n",
    "            map_details['t_side_t1'].append(t_side_t1)\n",
    "            map_details['team_2'].append(team_2)\n",
    "            map_details['score_t2'].append(score_t2)\n",
    "            map_details['ct_side_t2'].append(ct_side_t2)\n",
    "            map_details['t_side_t2'].append(t_side_t2)\n",
    "            map_details['map'].append(map_)\n",
    "            map_details['best_of'].append(best_of)\n",
    "            map_details['team_start_ct'].append(team_starts_ct)\n",
    "            map_details['team_1_rating'].append(team1_rating)\n",
    "            map_details['team_2_rating'].append(team2_rating)\n",
    "            map_details['team_1_first_kill'].append(team1_first_kill)\n",
    "            map_details['team_2_first_kill'].append(team2_first_kill)\n",
    "            map_details['team_1_clutches'].append(team1_clutches)\n",
    "            map_details['team_2_clutches'].append(team2_clutches)\n",
    "            map_details['eco_t1'].append(eco_t1)\n",
    "            map_details['eco_t2'].append(eco_t2)\n",
    "            map_details['semi_eco_t1'].append(semi_eco_t1)\n",
    "            map_details['semi_eco_t2'].append(semi_eco_t2)\n",
    "            map_details['semi_buy_t1'].append(semi_buy_t1)\n",
    "            map_details['semi_buy_t2'].append(semi_buy_t2)\n",
    "            map_details['full_buy_t1'].append(full_buy_t1)\n",
    "            map_details['full_buy_t2'].append(full_buy_t2)\n",
    "            \n",
    "        except:\n",
    "            links_quebrados.append(url)\n",
    "            print('quebrou')\n",
    "            time.sleep(5)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "df_map_details.append(pd.DataFrame(map_details))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get economy informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_map_details[3]\n",
    "\n",
    "#economy_details = []\n",
    "\n",
    "df = pd.concat(df_map_details)\n",
    "\n",
    "df.to_csv('dataframe_csgo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [22:43<00:00,  4.55s/it]\n"
     ]
    }
   ],
   "source": [
    "economy = {\n",
    "    'match_id': [],\n",
    "    'map_1' : [],\n",
    "    'map_2' : [],\n",
    "    'map_3' : [],    \n",
    "    'eco_t1' : [],\n",
    "    'eco_t2' : [],\n",
    "    'semi_eco_t1' : [],\n",
    "    'semi_eco_t2' : [],\n",
    "    'semi_buy_t1' : [],\n",
    "    'semi_buy_t2' : [],\n",
    "    'full_buy_t1' : [],\n",
    "    'full_buy_t2' : [],\n",
    "}\n",
    "for item in tqdm(map_details_links[4][:300]):\n",
    "    url =f'https://www.hltv.org{item}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: \n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html)\n",
    "        #try:\n",
    "            eco_t1 = soup.find_all('div',{'class':'col standard-box stats-rows'})[0]('span',{'title':'Played'})[0].text.split()[0]\n",
    "            eco_t2 = soup.find_all('div',{'class':'col standard-box stats-rows'})[1]('span',{'title':'Played'})[0].text.split()[0]\n",
    "            semi_eco_t1 = soup.find_all('div',{'class':'col standard-box stats-rows'})[0]('span',{'title':'Played'})[1].text.split()[0]\n",
    "            semi_eco_t2 = soup.find_all('div',{'class':'col standard-box stats-rows'})[1]('span',{'title':'Played'})[1].text.split()[0]\n",
    "            semi_buy_t1 = soup.find_all('div',{'class':'col standard-box stats-rows'})[0]('span',{'title':'Played'})[2].text.split()[0]\n",
    "            semi_buy_t2 = soup.find_all('div',{'class':'col standard-box stats-rows'})[1]('span',{'title':'Played'})[2].text.split()[0]\n",
    "            full_buy_t1 = soup.find_all('div',{'class':'col standard-box stats-rows'})[0]('span',{'title':'Played'})[3].text.split()[0]\n",
    "            full_buy_t2 = soup.find_all('div',{'class':'col standard-box stats-rows'})[1]('span',{'title':'Played'})[3].text.split()[0]\n",
    "            match_id = df_map_links[df_map_links['economy1'] == map_details_links[4][1]]['match_id'][1]\n",
    "            tempurl = soup.find_all('div',{'class':'tabs'})[0]('a',{'class':'stats-top-menu-item stats-top-menu-item-link'})[0]['href']\n",
    "\n",
    "            economy['map_1'].append(map_1)\n",
    "            economy['map_2'].append(map_2)\n",
    "            economy['map_3'].append(map_3)\n",
    "            economy['match_id'].append(match_id)\n",
    "            economy['eco_t1'].append(eco_t1)\n",
    "            economy['eco_t2'].append(eco_t2)\n",
    "            economy['semi_eco_t1'].append(semi_eco_t1)\n",
    "            economy['semi_eco_t2'].append(semi_eco_t2)\n",
    "            economy['semi_buy_t1'].append(semi_buy_t1)\n",
    "            economy['semi_buy_t2'].append(semi_buy_t2)\n",
    "            economy['full_buy_t1'].append(full_buy_t1)\n",
    "            economy['full_buy_t2'].append(full_buy_t2)\n",
    "            time.sleep(4)\n",
    "\n",
    "            tempurl = soup.find_all('div',{'class':'tabs'})[0]('a',{'class':'stats-top-menu-item stats-top-menu-item-link'})[0]['href']\n",
    "            url2 =f'https://www.hltv.org{tempurl}'\n",
    "            response = requests.get(url2)\n",
    "            if response.status_code == 200:\n",
    "                html = response.content\n",
    "                soup = BeautifulSoup(html)\n",
    "        #except:\n",
    "            #pass\n",
    "    else:\n",
    "        pass\n",
    "        print('ok')\n",
    "economy_details.append(pd.DataFrame(economy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/stats/matches/economy/mapstatsid/144643/g2-vs-natus-vincere'"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "url =f'https://www.hltv.org/stats/matches/mapstatsid/144643/g2-vs-natus-vincere'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "soup.find_all('div',{'class':'tabs'})[0]('a',{'class':'stats-top-menu-item stats-top-menu-item-link'})[1]['href']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e04447ceff53b0f1659ba531c0d619aef550c6c7745f8d024ec173fc8d7f9a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
